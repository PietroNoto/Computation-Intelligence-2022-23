<h1>Overview</h1>

<p>
My algorithm starts from the Genetic approach, based on creating a population with a certain number of individuals, each carrying a genome, in which each gene is 0 or 1. Then we let each individual evolve, producing its offspring which is randomly affected by mutations (prob. = 30%).


<h1>Application of the GA approach</h1>

<p>
A population is made of a certain number of individuals that I treated as a hyperparameter; each individual carries a genome, which works as a bitmap for the dictionary of the lists generated by the problem() function.
Example: an individual with genome (1, 0, 0, 1) means that our potential set includes list 0 and 3 and discards list 1 and 2. Of course, the genome size is equal to the number of lists generated by the problem(), after removing all the duplicates.
Once the population is created and left to evolution , the algorithms examines each individual and picks the ones whose bitmaps make a covering set. It doesn't simply pick the first solution, but compares all individuals in order to find the solution with the minimum number of elements.
</p>


<h1>Performance analysis</h1>

<p>
I noticed that the bigger the population size the most likely it has individuals with great potential to solve the problem and find more optimal solutions (just like in biology...).
Besides, the more populations are created, the higher the chances to find fitter individuals: instead of creating one single population made of POPULATION_SIZE individuals, it's better to create EPOCH populations of the same number of individuals. The higher EPOCH is the more optimal the solutions are at the price of higher computation time.
So I treated POPULATION_SIZE (P) as a hyperparameter alongside OFFSPRING_SIZE (O).
The following tests have been performed (for reference I simply considered N = 10 and EPOCH = 10):
    * P = 20:
        + O = 1 -> n. of elements = 32
        + O = 5 -> n. of elements = 36
        + O = 10 -> n. of elements = 34
    * P = 50:
        + O = 1 -> n. of elements = 27
        + O = 5 -> n. of elements = 29
        + O = 10 -> n. of elements = 33
    * P = 100:
        + O = 1 -> n. of elements = 32
        + O = 5 -> n. of elements = 25
        + O = 10 -> n. of elements = 34
    * P = 200:
        + O = 1 -> n. of elements = 31
        + O = 5 -> n. of elements = 34
        + O = 10 -> n. of elements = 32
    * P = 500:
        + O = 1 -> n. of elements = 32
        + O = 5 -> n. of elements = 33
        + O = 10 -> n. of elements = 31
    * P = 1000:
        + O = 1 -> n. of elements = 27
        + O = 5 -> n. of elements = 27
        + O = 10 -> n. of elements = 25
    * P = 10000:
        + O = 1 -> n. of elements = 25
        + O = 5 -> n. of elements = 21
        + O = 10 -> n. of elements = 21
    * P = 10^5:
        + O = 1 -> n. of elements = 18
        + O = 5 -> n. of elements = 19
        + O = 10 -> n. of elements = 21

    The evaluation phase has been performed using POPULATION_SIZE = 10^4 but increasing EPOCHS to 100, obtaining the following results:

    * N = 5: n. of elements = 5, total steps = 102396
    * N = 10: n. of elements = 17, total steps = 1093266 (10 times slower than N = 5)
    * N = 20: n. of elements = 31, total steps = 1091916 (approx eq. to N = 10)
    * N > 20: n. of elements < inf but no solution found!



</p>


<h1>Comparison with Lab1 algorithm</h1>

<p>
Despite my lab1 solution uses combinations, whose solution space (and time) grows exponentially with problem size, this solution based on GA is definitely slower and less effective: it is capable to solve the Set cover problem till N = 20 versus N = 40 of combinations-based solution.
Here a quick recap of lab1 solution to make a comparison:

* N = 5, elements: 5, visited nodes: 6
* N = 10, best solution: (2, 26, 35), elements: 10, visited nodes: 2230
* N = 20, best solution: (4, 18, 19, 21, 23), elements: 23, visited nodes: 330616
* N = 30, best solution: (4, 18, 33, 73, 101), elements: 36, visited nodes: 134375527
* N = 40, best solution: (9, 11, 45, 50, 53), elements: 54, visited nodes: 11288629

With GA the amount of steps needed starts pretty high, but grows slightly with N; with combinations it starts very low but grows exponentially and it does take too long to find a solution for N > 40.
</p>


<h1>Edits</h1>

<p>
I noticed that the lower the fitness (sum of ones of a genome) the closer the individual is to the optimum, because fewer lists from the dictionary are selected. That's why I changed genetic_algorithm.py::line64 reverse sort from True to False, this way I get individuals with lowest fitness first.
Additionally, I preferred not to trim the population because of massive amount of duplicates that kill potential wipe out potentially good individuals. Therefore, I applied a duplicate removal task afterwards, once the population has been created.
</p>